{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":9954617,"sourceType":"datasetVersion","datasetId":6122271}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nbase_path = '/kaggle/input/cleanedtabularcsv/cleaned_train.csv'\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Load the cleaned training data\ntrain = pd.read_csv(base_path)\ntrain.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:27.362351Z","iopub.execute_input":"2024-11-22T18:39:27.362727Z","iopub.status.idle":"2024-11-22T18:39:30.424498Z","shell.execute_reply.started":"2024-11-22T18:39:27.362681Z","shell.execute_reply":"2024-11-22T18:39:30.423476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dropping a column by its name, since these columns were just used for EDA purpose and won't play any role in model training\ntrain.drop(['age_group','id'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:30.426154Z","iopub.execute_input":"2024-11-22T18:39:30.426655Z","iopub.status.idle":"2024-11-22T18:39:30.437541Z","shell.execute_reply.started":"2024-11-22T18:39:30.426616Z","shell.execute_reply":"2024-11-22T18:39:30.436299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking the correlation of different features, to see if more features can be dropped\ncorrelation_matrix = train.corr()\ncorrelation_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:30.438812Z","iopub.execute_input":"2024-11-22T18:39:30.439219Z","iopub.status.idle":"2024-11-22T18:39:30.468890Z","shell.execute_reply.started":"2024-11-22T18:39:30.439174Z","shell.execute_reply":"2024-11-22T18:39:30.467734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the correlation matrix as a heatmap to understand the correlation\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True)\nplt.title(\"Correlation Matrix Heatmap\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:30.471752Z","iopub.execute_input":"2024-11-22T18:39:30.472184Z","iopub.status.idle":"2024-11-22T18:39:31.149418Z","shell.execute_reply.started":"2024-11-22T18:39:30.472139Z","shell.execute_reply":"2024-11-22T18:39:31.148306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the scatter plot between the most correlated features with the target variable to understand the distribution and behaviour of target variable with the feature variable\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(train['PreInt_EduHx-computerinternet_hoursday'], train['SDS-SDS_Total_T'], c=train['sii'], cmap=\"viridis\", s=100, edgecolor=\"k\")\nplt.xlabel('internet hours per day', fontsize=12)\nplt.ylabel('sleep distrurbance', fontsize=12)\nplt.title(f\"2D Scatter Plot of PreInt_EduHx-computerinternet_hoursday vs SDS-SDS_Total_T (colored by sii)\", fontsize=14)\nplt.colorbar(scatter, label=f\"SII Value\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:31.150802Z","iopub.execute_input":"2024-11-22T18:39:31.151137Z","iopub.status.idle":"2024-11-22T18:39:31.599035Z","shell.execute_reply.started":"2024-11-22T18:39:31.151104Z","shell.execute_reply":"2024-11-22T18:39:31.598043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking only the most 3 top correlated features with the target variable to understand the relationship\nx_feature = \"PreInt_EduHx-computerinternet_hoursday\"\ny_feature = \"SDS-SDS_Total_T\"\nz_feature = \"Basic_Demos-Age\"\ntarget = \"sii\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:31.600285Z","iopub.execute_input":"2024-11-22T18:39:31.600592Z","iopub.status.idle":"2024-11-22T18:39:31.605391Z","shell.execute_reply.started":"2024-11-22T18:39:31.600562Z","shell.execute_reply":"2024-11-22T18:39:31.604322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plotting the scatter plots for them\nfrom matplotlib import cm\n#discrete colormap\nunique_sii_values = sorted(train[target].unique())  # Unique sii values\nnum_colors = len(unique_sii_values)\ncmap = cm.get_cmap(\"tab10\", num_colors)  # Use a discrete colormap\n\n# Mapping sii values to indices for coloring\ncolor_indices = [unique_sii_values.index(value) for value in train[target]]\n\n# a 3D scatter plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\nscatter = ax.scatter(\n    train[x_feature], \n    train[y_feature], \n    train[z_feature], \n    c=color_indices,  # Used mapped indices for colors\n    cmap=cmap, \n    s=100, \n    edgecolor=\"k\"\n)\n\nax.set_xlabel(x_feature, fontsize=12)\nax.set_ylabel(y_feature, fontsize=12)\nax.set_zlabel(z_feature, fontsize=12)\nax.set_title(f\"3D Scatter Plot of {x_feature}, {y_feature}, and {z_feature} (colored by {target})\", fontsize=14)\n\ncbar = plt.colorbar(scatter, ax=ax, pad=0.1)\ncbar.set_ticks(range(num_colors))\ncbar.set_ticklabels(unique_sii_values)  # Map ticks to sii values\ncbar.set_label(f\"{target} Value\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:31.607206Z","iopub.execute_input":"2024-11-22T18:39:31.607586Z","iopub.status.idle":"2024-11-22T18:39:32.123066Z","shell.execute_reply.started":"2024-11-22T18:39:31.607556Z","shell.execute_reply":"2024-11-22T18:39:32.122054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\n\n# trying to work only with the most relevant columns\nfeatures = [\"PreInt_EduHx-computerinternet_hoursday\", \"SDS-SDS_Total_T\", \"Basic_Demos-Age\", \"sii\"]\n\n# the pairplot with hue based on the 'sii' classes\nsns.pairplot(train[features], hue=\"sii\", palette=\"tab10\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:32.124829Z","iopub.execute_input":"2024-11-22T18:39:32.125197Z","iopub.status.idle":"2024-11-22T18:39:36.253434Z","shell.execute_reply.started":"2024-11-22T18:39:32.125156Z","shell.execute_reply":"2024-11-22T18:39:36.252253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A pairplot (scatterplot matrix) will help us in visualizing the pairwise relationships between all features. It can help identify which features have a better chance of separating the classes based on their pairwise relationships","metadata":{}},{"cell_type":"code","source":"# Checking distribution of target variable 'sii'\nclass_counts = train[\"sii\"].value_counts()\nprint(class_counts)\nclass_counts.plot(kind='bar', figsize=(6, 4), color='skyblue', edgecolor='black')\nplt.title('Distribution of Target Variable (sii)')\nplt.xlabel('sii Value')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:36.255014Z","iopub.execute_input":"2024-11-22T18:39:36.255915Z","iopub.status.idle":"2024-11-22T18:39:36.528198Z","shell.execute_reply.started":"2024-11-22T18:39:36.255863Z","shell.execute_reply":"2024-11-22T18:39:36.527149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our dataset is very imbalanced and their is a impact of Imbalanced Data that is model bias. Our ML algo will be biased toward the majority class and will predict the majority class most of the time, leading to poor performance for the minority classes.\nSince accuracy alone will not give a good indication of model performance when dealing with imbalanced datasets, we can use other metrics like Precision, Recall, and F1-Score\nConfusion Matrix\nROC-AUC for multi-class classification","metadata":{}},{"cell_type":"markdown","source":"To handle imbalanced dataset, we can do resampling to make the dataset more balanced.\nSince our data is already very limited, let's only try Oversampling (SMOTE)","metadata":{}},{"cell_type":"code","source":"pip install imbalanced-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:36.531501Z","iopub.execute_input":"2024-11-22T18:39:36.531829Z","iopub.status.idle":"2024-11-22T18:39:47.751631Z","shell.execute_reply.started":"2024-11-22T18:39:36.531797Z","shell.execute_reply":"2024-11-22T18:39:47.750317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nX = train.drop(columns=[\"sii\"])\ny = train[\"sii\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\nprint(f\"Before SMOTE: {y_train.value_counts()}\")\nprint(f\"After SMOTE: {y_resampled.value_counts()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:47.753302Z","iopub.execute_input":"2024-11-22T18:39:47.753655Z","iopub.status.idle":"2024-11-22T18:39:48.420216Z","shell.execute_reply.started":"2024-11-22T18:39:47.753617Z","shell.execute_reply":"2024-11-22T18:39:48.419186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"target variable (sii) after SMOTE to ensure that the classes are more balanced:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Checking the distribution of the target variable after SMOTE\nplt.figure(figsize=(8, 6))\ny_resampled.value_counts().plot(kind=\"bar\", color=\"skyblue\")\nplt.title(\"Target Variable Distribution After SMOTE\", fontsize=14)\nplt.xlabel(\"Classes\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.xticks(rotation=0)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:48.421815Z","iopub.execute_input":"2024-11-22T18:39:48.422990Z","iopub.status.idle":"2024-11-22T18:39:48.670024Z","shell.execute_reply.started":"2024-11-22T18:39:48.422941Z","shell.execute_reply":"2024-11-22T18:39:48.668994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualizing the Decision Boundary","metadata":{}},{"cell_type":"markdown","source":"using techniques like PCA or t-SNE, we can try to find the hidden behaviour of features and understand how decision boundaries will look like in 2D","metadata":{}},{"cell_type":"markdown","source":"Dimensionality Reduction with PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Apply PCA to reduce the feature space to 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_resampled)\n\n# Plotting the decision boundary in the PCA space\nfrom sklearn.svm import SVC\nimport numpy as np\n\n# Training an SVM classifier\nsvm_model = SVC(kernel='linear', random_state=42)\nsvm_model.fit(X_pca, y_resampled)\n\n# Create a grid to plot decision boundaries\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n\n# Predict on each point of the grid\nZ = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_resampled, edgecolor='k', cmap=\"viridis\", s=100)\nplt.title(\"Decision Boundary using PCA\", fontsize=14)\nplt.xlabel(\"PCA Component 1\", fontsize=12)\nplt.ylabel(\"PCA Component 2\", fontsize=12)\nplt.colorbar(label=\"Class\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:39:48.671434Z","iopub.execute_input":"2024-11-22T18:39:48.671715Z","iopub.status.idle":"2024-11-22T18:40:37.378780Z","shell.execute_reply.started":"2024-11-22T18:39:48.671687Z","shell.execute_reply":"2024-11-22T18:40:37.377784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now trying with t-SNE","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# Apply t-SNE to reduce to 2D\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_resampled)\n\n# Train an SVM classifier\nsvm_model = SVC(kernel='linear', random_state=42)\nsvm_model.fit(X_tsne, y_resampled)\n\n# Create a grid to plot decision boundaries\nx_min, x_max = X_tsne[:, 0].min() - 1, X_tsne[:, 0].max() + 1\ny_min, y_max = X_tsne[:, 1].min() - 1, X_tsne[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n\n# Predict on each point of the grid\nZ = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_resampled, edgecolor='k', cmap=\"viridis\", s=100)\nplt.title(\"Decision Boundary using t-SNE\", fontsize=14)\nplt.xlabel(\"t-SNE Component 1\", fontsize=12)\nplt.ylabel(\"t-SNE Component 2\", fontsize=12)\nplt.colorbar(label=\"Class\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:40:37.380211Z","iopub.execute_input":"2024-11-22T18:40:37.380622Z","iopub.status.idle":"2024-11-22T18:43:42.236144Z","shell.execute_reply.started":"2024-11-22T18:40:37.380579Z","shell.execute_reply":"2024-11-22T18:43:42.235064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"as we can see data is not linearly separable, we can try other types of kernels in the Support Vector Machine (SVM)","metadata":{}},{"cell_type":"markdown","source":"Radial Basis Function (RBF) Kernel (Gaussian Kernel) can be tried out as I suspect that the decision boundary is non-linear","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Train an SVM classifier with RBF kernel\nsvm_model_rbf = SVC(kernel='rbf', random_state=42)\nsvm_model_rbf.fit(X_tsne, y_resampled)\n\n# Create a grid to plot decision boundaries\nx_min, x_max = X_tsne[:, 0].min() - 1, X_tsne[:, 0].max() + 1\ny_min, y_max = X_tsne[:, 1].min() - 1, X_tsne[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n\n# Predict on each point of the grid\nZ = svm_model_rbf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_resampled, edgecolor='k', cmap=\"viridis\", s=100)\nplt.title(\"Decision Boundary using RBF Kernel\", fontsize=14)\nplt.xlabel(\"t-SNE Component 1\", fontsize=12)\nplt.ylabel(\"t-SNE Component 2\", fontsize=12)\nplt.colorbar(label=\"Class\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:43:42.237570Z","iopub.execute_input":"2024-11-22T18:43:42.237921Z","iopub.status.idle":"2024-11-22T18:49:17.194300Z","shell.execute_reply.started":"2024-11-22T18:43:42.237888Z","shell.execute_reply":"2024-11-22T18:49:17.193318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Trying polynomial Kernel","metadata":{}},{"cell_type":"code","source":"# Train an SVM classifier with Polynomial kernel\nsvm_model_poly = SVC(kernel='poly', degree=3, random_state=42)\nsvm_model_poly.fit(X_tsne, y_resampled)\n\n# Predict on each point of the grid\nZ = svm_model_poly.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_resampled, edgecolor='k', cmap=\"viridis\", s=100)\nplt.title(\"Decision Boundary using Polynomial Kernel\", fontsize=14)\nplt.xlabel(\"t-SNE Component 1\", fontsize=12)\nplt.ylabel(\"t-SNE Component 2\", fontsize=12)\nplt.colorbar(label=\"Class\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:49:17.196321Z","iopub.execute_input":"2024-11-22T18:49:17.196745Z","iopub.status.idle":"2024-11-22T18:51:49.756581Z","shell.execute_reply.started":"2024-11-22T18:49:17.196702Z","shell.execute_reply":"2024-11-22T18:51:49.755595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SVC uses“one-versus-one” approach for multi-class classification\nIn total, 4 * (4 - 1) / 2 = 6 classifiers are constructed and each one trains data from two classes.","metadata":{}},{"cell_type":"markdown","source":"LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_classes models.","metadata":{}},{"cell_type":"markdown","source":"trying with Sigmoid","metadata":{}},{"cell_type":"code","source":"# Train an SVM classifier with Sigmoid kernel\nsvm_model_sigmoid = SVC(kernel='sigmoid', random_state=42)\nsvm_model_sigmoid.fit(X_tsne, y_resampled)\n\n# Predict on each point of the grid\nZ = svm_model_sigmoid.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_resampled, edgecolor='k', cmap=\"viridis\", s=100)\nplt.title(\"Decision Boundary using Sigmoid Kernel\", fontsize=14)\nplt.xlabel(\"t-SNE Component 1\", fontsize=12)\nplt.ylabel(\"t-SNE Component 2\", fontsize=12)\nplt.colorbar(label=\"Class\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:51:49.757892Z","iopub.execute_input":"2024-11-22T18:51:49.758214Z","iopub.status.idle":"2024-11-22T18:57:34.216390Z","shell.execute_reply.started":"2024-11-22T18:51:49.758183Z","shell.execute_reply":"2024-11-22T18:57:34.215296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tuning the Hyperparameters","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\r\n\r\n# Define the parameter grid for tuning\r\nparam_grid = {\r\n    'C': [0.1, 1, 10],\r\n    'gamma': ['scale', 'auto', 0.1, 1],'kernel': ['moid'\r\n}\r\n\r\n# Perform GridSearchCV for hyperparameter tuning\r\ngrid_search = GridSearchCV(SVC(), param_grid, cv=3, verbose=1)\r\ngrid_search.fit(X_tsne, y_resampled)\r\n\r\n# Best parameters and score\r\nprint(\"Best Parameters:\", grid_search.best_params_)\r\nprint(\"Best Score:\", grid_search.best_score_)\r\n\r\n# Use the best model to predict and plot the decision boundary\r\nbest_model = grid_search.best_estimator_\r\nZ = best_model.predict(np.c_[xx.ravel(), yy.ravel()])\r\nZ = Z.reshape(xx.shape)\r\n\r\nplt.figure(figsize=(8, 6))\r\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\r\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_resampled, edgecolor='k', cmap=\"viridis\", s=100)\r\nplt.title(\"Decision Boundary with Best SVM Model (after tuning)\", fontsize=14)\r\nplt.xlabel(\"t-SNE Component 1\", fontsize=12)\r\nplt.ylabel(\"t-SNE Component 2\", fontsize=12)\r\nplt.colorbar(label=\"Class\")\r\nplt.show()\r\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SMOTE vs. Class Weights\nI though to check this, \"If you're using class weights in SVM, you generally don't need SMOTE unless the performance is still not satisfactory and you want to further balance the dataset before training.\nIf the class imbalance is severe or class weights alone don't yield good performance, then SMOTE is a good alternative to improve the model.\"\nso I tried to experiment with both methods and choose the one (or combination) that provides the best results for your specific case.","metadata":{}},{"cell_type":"markdown","source":"Class Weights in SVM (Without SMOTE)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\n\n# Define X (features) and y (target)\nX = train.drop('sii', axis=1)  # Exclude the target variable\ny = train['sii']\n\n# Split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Get the class distribution (to calculate class weights)\nsii_dist = y_train.value_counts()\n\n# Calculate class weights (inverse proportional to class frequency)\nclass_weights = {class_label: 1 / (count / len(y_train)) for class_label, count in sii_dist.items()}\n\n# Train an SVM classifier with class weights\nsvm_model_weighted = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\n\n# Fit the model\nsvm_model_weighted.fit(X_train, y_train)\n\n# Predict and evaluate the model\ny_pred_weighted = svm_model_weighted.predict(X_test)\nprint(\"Classification Report for SVM with Class Weights:\")\nprint(classification_report(y_test, y_pred_weighted))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:57:34.217695Z","iopub.execute_input":"2024-11-22T18:57:34.217981Z","iopub.status.idle":"2024-11-22T18:57:34.572380Z","shell.execute_reply.started":"2024-11-22T18:57:34.217954Z","shell.execute_reply":"2024-11-22T18:57:34.571311Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Metrics:\n\nClass 0.0: Precision = 0.74, Recall = 0.67, F1-score = 0.70\nClass 1.0: Precision = 0.40, Recall = 0.28, F1-score = 0.33\nClass 2.0: Precision = 0.18, Recall = 0.26, F1-score = 0.21\nClass 3.0: Precision = 0.11, Recall = 0.71, F1-score = 0.20\nStrengths:\n\nClass 0.0: The precision and recall are decent, indicating that this class is relatively well-predicted.\nClass 3.0: The high recall (0.71) suggests that it's being correctly identified, but precision is low, implying many false positives.\nWeaknesses:\n\nClasses 1.0 and 2.0: Both have low precision and recall, indicating that the model struggles to distinguish these classes well.\nMacro avg: Precision = 0.36, Recall = 0.48, F1-score = 0.36 (indicating the model struggles with the minority classes overall).","metadata":{}},{"cell_type":"markdown","source":"SMOTE + SVM (With SMOTE)\nuse SMOTE to oversample the minority classes in the training set and then train the SVM on the balanced data.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\n\n\n# Define X (features) and y (target)\nX = train.drop('sii', axis=1)  # Exclude the target variable\ny = train['sii']\n\n# Split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to oversample the minority classes\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Train an SVM classifier (without class weights) on the SMOTE balanced data\nsvm_model_smote = SVC(kernel='rbf', random_state=42)\n\n# Fit the model\nsvm_model_smote.fit(X_train_smote, y_train_smote)\n\n# Predict and evaluate the model\ny_pred_smote = svm_model_smote.predict(X_test)\nprint(\"Classification Report for SVM with SMOTE:\")\nprint(classification_report(y_test, y_pred_smote))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:57:34.573728Z","iopub.execute_input":"2024-11-22T18:57:34.574055Z","iopub.status.idle":"2024-11-22T18:57:35.820507Z","shell.execute_reply.started":"2024-11-22T18:57:34.574025Z","shell.execute_reply":"2024-11-22T18:57:35.819457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Metrics:\n\nClass 0.0: Precision = 0.71, Recall = 0.67, F1-score = 0.69\nClass 1.0: Precision = 0.39, Recall = 0.20, F1-score = 0.26\nClass 2.0: Precision = 0.17, Recall = 0.28, F1-score = 0.21\nClass 3.0: Precision = 0.11, Recall = 0.86, F1-score = 0.19\nStrengths:\n\nClass 3.0: SMOTE improves recall significantly for this class (0.86), but precision is still low (0.11).\nClass 0.0: Precision is still relatively good, but recall has slightly decreased compared to the previous model.\nWeaknesses:\n\nClass 1.0: Recall drops to 0.20 (even worse than in the class weights case), which means the model is even worse at identifying this class.\nClass 2.0: While recall has improved slightly, precision remains low (0.17).\nMacro avg: Precision = 0.35, Recall = 0.50, F1-score = 0.34 (still showing issues with minority classes).","metadata":{}},{"cell_type":"markdown","source":"Combining Both: SMOTE + Class Weights in SVM\nThe combination of both methods, apply SMOTE first and then use class weights when training the SVM model.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\n\n# Define X (features) and y (target)\nX = train.drop('sii', axis=1)  # Exclude the target variable\ny = train['sii']\n\n# Split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to oversample the minority classes\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Get the class distribution after SMOTE (for calculating class weights)\nsii_dist_smote = y_train_smote.value_counts()\n\n# Calculate class weights (inverse proportional to class frequency)\nclass_weights_smote = {class_label: 1 / (count / len(y_train_smote)) for class_label, count in sii_dist_smote.items()}\n\n# Train an SVM classifier with class weights on the SMOTE balanced data\nsvm_model_smote_weighted = SVC(kernel='rbf', class_weight=class_weights_smote, random_state=42)\n\n# Fit the model\nsvm_model_smote_weighted.fit(X_train_smote, y_train_smote)\n\n# Predict and evaluate the model\ny_pred_smote_weighted = svm_model_smote_weighted.predict(X_test)\nprint(\"Classification Report for SVM with SMOTE and Class Weights:\")\nprint(classification_report(y_test, y_pred_smote_weighted))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:57:35.821790Z","iopub.execute_input":"2024-11-22T18:57:35.822098Z","iopub.status.idle":"2024-11-22T18:57:37.039205Z","shell.execute_reply.started":"2024-11-22T18:57:35.822068Z","shell.execute_reply":"2024-11-22T18:57:37.037975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Metrics:\n\nClass 0.0: Precision = 0.73, Recall = 0.67, F1-score = 0.70\nClass 1.0: Precision = 0.41, Recall = 0.26, F1-score = 0.32\nClass 2.0: Precision = 0.21, Recall = 0.31, F1-score = 0.25\nClass 3.0: Precision = 0.11, Recall = 0.86, F1-score = 0.19\nStrengths:\n\nClass 0.0: Precision is still strong (0.73), and recall remains steady (0.67).\nClass 3.0: Recall remains high (0.86), indicating that this class is still being identified correctly, though precision is low.\nClass 1.0 and 2.0: There's a slight improvement in recall and precision compared to SMOTE-only, though not dramatic.\nWeaknesses:\n\nClass 3.0: The precision still remains low (0.11), which means a lot of false positives.\nClass 1.0 and 2.0: These classes still face challenges with low precision and recall, though the combined approach seems to help slightly.\nMacro avg: Precision = 0.36, Recall = 0.52, F1-score = 0.37 (overall, these results are still not stellar).","metadata":{}},{"cell_type":"markdown","source":"Class Weights: The class_weight parameter is set based on the distribution of the classes. SVM adjusts the decision boundary by assigning different penalties for misclassifying different classes.\nSMOTE: SMOTE is used to oversample the minority classes in the training data, generating synthetic samples, and then the SVM model is trained on the balanced dataset.\nSMOTE + Class Weights: First, SMOTE is applied to balance the dataset, and then class weights are used to fine-tune the SVM's decision boundary.","metadata":{}},{"cell_type":"markdown","source":"Comparative Summary:\nSVM with Class Weights:\n\nBest performance for Class 0.0, with reasonable recall and precision.\nStruggles with Class 1.0 and Class 2.0 but performs decently on Class 3.0 due to the class weights.\nMacro avg shows imbalanced performance with underperformance on the minority classes.\nSVM with SMOTE:\n\nClass 3.0 has significantly improved recall, but precision remains a concern.\nClass 1.0 and Class 2.0 suffer with low recall (indicating the model is still having difficulty with these minority classes).\nThe results show a slight improvement over the class weights approach, but still not optimal.\nSVM with SMOTE and Class Weights:\n\nClass 0.0: Solid performance, with similar metrics as the class weights-only approach.\nClass 1.0 and Class 2.0: There is some improvement in recall and precision, though it's still far from optimal.\nClass 3.0: Recall is still high (0.86), but precision remains problematic.\nOverall, this combination slightly improves the results over using SMOTE alone, but still, the class imbalance issue persists.","metadata":{}},{"cell_type":"markdown","source":"Combination of SMOTE and Class Weights seems to be the most effective approach, but it still struggles to achieve a well-balanced model across all classes, especially for Class 1.0 and Class 2.0.\nIf Class 3.0 is the most important (given its recall), this combination might work best.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Prepare data (replace X and y with your actual features and target variables)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# SMOTE with more focus on Class 3.0\nsmote = SMOTE(sampling_strategy={3.0: 500}, random_state=42)  # Adjust Class 3.0 samples\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# SVM with RBF kernel and class weights (adjusted for Class 3.0)\nclass_weights = {0.0: 1.0, 1.0: 2.0, 2.0: 3.0, 3.0: 10.0}  # Focus on Class 3.0\nsvm_model = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\n\n# Train the SVM model\nsvm_model.fit(X_resampled, y_resampled)\n\n# Predict on the test set\ny_pred = svm_model.predict(X_test)\n\n# Print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:57:37.040581Z","iopub.execute_input":"2024-11-22T18:57:37.040919Z","iopub.status.idle":"2024-11-22T18:57:37.383974Z","shell.execute_reply.started":"2024-11-22T18:57:37.040885Z","shell.execute_reply":"2024-11-22T18:57:37.383005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"updated results suggest that Class 3.0 is still being prioritized in terms of recall (0.70), but precision for Class 3.0 has dropped significantly to 0.03, indicating that most of the predictions for Class 3.0 are false positives. Additionally, the recall for Class 2.0 is quite low (0.02), and Class 1.0 has also seen a decrease in recall (0.14).\n\nWhile increasing the weight for Class 3.0 helped its recall, it led to too many false positives, resulting in very low precision. we can try reducing the class weight for Class 3.0 slightly to balance recall and precision. It's also worth experimenting with increasing the weight for Class 2.0 and Class 1.0 to improve recall for these classes.\n\nIt looks like Class 1.0 and Class 2.0 still have low recall values. You could experiment with increasing the number of samples generated by SMOTE for these classes. By doing so, the model will see more samples for these classes and might learn better decision boundarie","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Prepare data (replace X and y with your actual features and target variables)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# SMOTE with more focus on Class 3.0\nsmote = SMOTE(sampling_strategy={1.0: 503, 2.0: 503, 3.0: 503}, random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# SVM with RBF kernel and class weights (adjusted for Class 3.0)\nclass_weights = {0.0: 1.0, 1.0: 2.0, 2.0: 3.0, 3.0: 5.0}  # Reduce weight for Class 3.0 slightly\nsvm_model = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\n\n# Train the SVM model\nsvm_model.fit(X_resampled, y_resampled)\n\n# Predict on the test set\ny_pred = svm_model.predict(X_test)\n# Print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:57:37.385374Z","iopub.execute_input":"2024-11-22T18:57:37.385666Z","iopub.status.idle":"2024-11-22T18:57:37.815725Z","shell.execute_reply.started":"2024-11-22T18:57:37.385636Z","shell.execute_reply":"2024-11-22T18:57:37.814748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Overall this notebook, is just for understanding how to approach the model development, I will develop models in separate notebook","metadata":{}}]}