{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b42299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete. Output saved to C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/feature_extraction.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing Parquet files\n",
    "input_directory = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/cleaned_series_train.parquet\"\n",
    "output_csv = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/feature_extraction.csv\"\n",
    "\n",
    "# Function to extract features from a single DataFrame\n",
    "def extract_features(df):\n",
    "    features = {}\n",
    "    \n",
    "    # Statistical features\n",
    "    features['mean_X'] = df['X'].mean()\n",
    "    features['std_X'] = df['X'].std()\n",
    "    features['mean_Y'] = df['Y'].mean()\n",
    "    features['std_Y'] = df['Y'].std()\n",
    "    features['mean_Z'] = df['Z'].mean()\n",
    "    features['std_Z'] = df['Z'].std()\n",
    "    features['mean_enmo'] = df['enmo'].mean()\n",
    "    features['std_enmo'] = df['enmo'].std()\n",
    "    features['mean_anglez'] = df['anglez'].mean()\n",
    "    features['std_anglez'] = df['anglez'].std()\n",
    "    \n",
    "    # Activity features\n",
    "    features['percent_no_motion'] = (df['enmo'] == 0).sum() / len(df) * 100\n",
    "    features['percent_non_wear'] = (df['non-wear_flag'] == 1).sum() / len(df) * 100\n",
    "    \n",
    "    # Battery features\n",
    "    features['mean_battery_voltage'] = df['battery_voltage'].mean()\n",
    "    critical_threshold = 3.5  # Example threshold\n",
    "    features['percent_below_threshold'] = (df['battery_voltage'] < critical_threshold).sum() / len(df) * 100\n",
    "    \n",
    "    # Participant ID (if available)\n",
    "    features['participant_id'] = df['participant_id'].iloc[0] if 'participant_id' in df.columns else None\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Collected all features into a list\n",
    "all_features = []\n",
    "\n",
    "# Iterated through each Parquet file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        try:\n",
    "            # Read Parquet file\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Extracted features\n",
    "            features = extract_features(df)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Created a DataFrame from all features\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# Saved to a CSV file\n",
    "features_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Feature extraction complete. Output saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548b5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    try:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Checked for required columns\n",
    "        required_columns = [\"X\", \"Y\", \"Z\", \"enmo\", \"anglez\", \"battery_voltage\", \"non-wear_flag\", \"participant_id\"]\n",
    "        if not all(col in data.columns for col in required_columns):\n",
    "            raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "        \n",
    "        # Statistical features\n",
    "        features = {\n",
    "            \"participant_id\": data[\"participant_id\"].iloc[0],\n",
    "            \"mean_X\": data[\"X\"].mean(),\n",
    "            \"std_X\": data[\"X\"].std(),\n",
    "            \"mean_Y\": data[\"Y\"].mean(),\n",
    "            \"std_Y\": data[\"Y\"].std(),\n",
    "            \"mean_Z\": data[\"Z\"].mean(),\n",
    "            \"std_Z\": data[\"Z\"].std(),\n",
    "            \"mean_enmo\": data[\"enmo\"].mean(),\n",
    "            \"std_enmo\": data[\"enmo\"].std(),\n",
    "            \"mean_anglez\": data[\"anglez\"].mean(),\n",
    "            \"std_anglez\": data[\"anglez\"].std(),\n",
    "        }\n",
    "        \n",
    "        # Activity features\n",
    "        features[\"percentage_no_motion\"] = (data[\"enmo\"] == 0).mean() * 100 if \"enmo\" in data.columns else np.nan\n",
    "        features[\"percentage_non_wear\"] = (data[\"non-wear_flag\"] == 1).mean() * 100 if \"non-wear_flag\" in data.columns else np.nan\n",
    "        \n",
    "        # Battery features\n",
    "        features[\"mean_battery_voltage\"] = data[\"battery_voltage\"].mean() if \"battery_voltage\" in data.columns else np.nan\n",
    "        features[\"percentage_low_battery\"] = (\n",
    "            (data[\"battery_voltage\"] < BATTERY_THRESHOLD).mean() * 100\n",
    "            if \"battery_voltage\" in data.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8d4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature percentage_no_motion not found in the dataset, skipping plot.\n",
      "Feature percentage_non_wear not found in the dataset, skipping plot.\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "if not features_df.empty:\n",
    "    key_features = [\"mean_enmo\", \"mean_anglez\", \"percentage_no_motion\", \"percentage_non_wear\", \"mean_battery_voltage\"]\n",
    "    \n",
    "    for feature in key_features:\n",
    "        if feature in features_df.columns:  # Ensure feature exists\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.histplot(features_df[feature].dropna(), kde=True, bins=30, color=\"skyblue\")\n",
    "            plt.title(f\"Distribution of {feature}\")\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.savefig(os.path.join(output_graphs_dir, f\"{feature}_distribution.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"Feature {feature} not found in the dataset, skipping plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cef7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaded the Data\n",
    "import pandas as pd\n",
    "\n",
    "# Loaded a single participant's data (example)\n",
    "data = pd.read_parquet(\"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/cleaned_series_train.parquet/id=00f332d1.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee962977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computed basic statistics for each participant\n",
    "basic_stats = data.describe().T[['mean', 'std', 'min', 'max', '25%', '50%', '75%']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "327dc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created time-based features\n",
    "data['hour'] = pd.to_datetime(data['time_of_day']).dt.hour\n",
    "data['minute'] = pd.to_datetime(data['time_of_day']).dt.minute\n",
    "data['is_weekend'] = data['weekday'].apply(lambda x: 1 if x in [5, 6] else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2da4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Daily Aggregations\n",
    "data['date'] = pd.to_datetime(data['relative_date_PCIAT']).dt.date\n",
    "daily_aggregations = data.groupby('date').agg({\n",
    "    'X': ['mean', 'std', 'max', 'min'],\n",
    "    'Y': ['mean', 'std', 'max', 'min'],\n",
    "    'Z': ['mean', 'std', 'max', 'min'],\n",
    "    'enmo': ['mean', 'std', 'max', 'min'],\n",
    "    'anglez': ['mean', 'std', 'max', 'min'],\n",
    "    'light': ['mean', 'std', 'max', 'min'],\n",
    "    'battery_voltage': ['mean', 'std', 'max', 'min']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b4c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-wear statistics\n",
    "data['non_wear_duration'] = data['non-wear_flag'].sum()  # Count non-wear periods\n",
    "data['activity_ratio'] = 1 - data['non-wear_flag'].mean()  # Activity density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26feb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude of accelerometer data\n",
    "data['magnitude'] = (data['X']**2 + data['Y']**2 + data['Z']**2)**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7c75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_stats = data['magnitude'].agg(['mean', 'std', 'max', 'min', 'median'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03fc2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle statistics\n",
    "anglez_stats = data['anglez'].agg(['mean', 'std', 'max', 'min'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a1293bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light and battery voltage statistics\n",
    "light_stats = data['light'].agg(['mean', 'std', 'max', 'min'])\n",
    "battery_voltage_stats = data['battery_voltage'].agg(['mean', 'std', 'max', 'min'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fad445db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity intensity categorization\n",
    "data['activity_level'] = pd.cut(data['magnitude'], bins=[0, 0.5, 1.0, 1.5, 2.0], labels=['Low', 'Medium', 'High', 'Very High'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b89b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing data\n",
    "data.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c39fb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Define the directory where your parquet files are stored\n",
    "directory = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/cleaned_series_train.parquet\"\n",
    "\n",
    "# Collect all parquet file paths in that directory\n",
    "participant_files = glob.glob(f\"{directory}*.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f65acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(participant_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "886366ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant_file in participant_files:\n",
    "    participant_data = pd.read_parquet(participant_file)\n",
    "    print(participant_data.head())  # Check the first few rows of the data\n",
    "    participant_features = extract_features(participant_data)\n",
    "    features_list.append(participant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55cf250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step         X         Y         Z      enmo     anglez  non-wear_flag  \\\n",
      "0     0  0.021536  0.022214 -1.022370  0.022853 -88.280762            0.0   \n",
      "1     1  0.022005  0.022187 -1.019740  0.020231 -88.241707            0.0   \n",
      "2     2  0.022240  0.022005 -1.019401  0.019893 -88.170067            0.0   \n",
      "3     3  0.021589  0.022578 -1.018177  0.018667 -88.250031            0.0   \n",
      "0     0  0.679618 -0.578170  0.320939  0.273671  18.857922            0.0   \n",
      "\n",
      "       light  battery_voltage     time_of_day  weekday  quarter  \\\n",
      "0  53.000000      4188.000000  56940000000000        4        3   \n",
      "1  51.666668      4188.166504  56945000000000        4        3   \n",
      "2  50.333332      4188.333496  56950000000000        4        3   \n",
      "3  50.500000      4188.500000  56955000000000        4        3   \n",
      "0   6.000000      4175.000000  40260000000000        2        3   \n",
      "\n",
      "   relative_date_PCIAT   day_time  \n",
      "0                 41.0  41.659028  \n",
      "1                 41.0  41.659086  \n",
      "2                 41.0  41.659144  \n",
      "3                 41.0  41.659201  \n",
      "0                 68.0  68.465972  \n"
     ]
    }
   ],
   "source": [
    "participant_data = pd.read_parquet(\"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/cleaned_series_train.parquet\")\n",
    "print(participant_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b51ae10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant_file in participant_files:\n",
    "    participant_data = pd.read_parquet(participant_file)\n",
    "    participant_features = extract_features(participant_data)\n",
    "    if participant_features is not None and not participant_features.empty:\n",
    "        features_list.append(participant_features)\n",
    "    else:\n",
    "        print(f\"Skipping {participant_file} due to empty features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac6c6551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features to concatenate.\n"
     ]
    }
   ],
   "source": [
    "if features_list:\n",
    "    final_features_df = pd.concat(features_list, axis=0)\n",
    "else:\n",
    "    print(\"No features to concatenate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca1fcdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features to save. Check the feature extraction process.\n"
     ]
    }
   ],
   "source": [
    "# Check if the features_list is populated\n",
    "if features_list:\n",
    "    final_features_df = pd.concat(features_list, axis=0)\n",
    "    # Save the extracted features\n",
    "    final_features_df.to_parquet(\"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project//extracted_features.parquet\")\n",
    "else:\n",
    "    print(\"No features to save. Check the feature extraction process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79802012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature sets: 0\n",
      "Feature list is empty.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of feature sets: {len(features_list)}\")\n",
    "if len(features_list) > 0:\n",
    "    final_features_df = pd.concat(features_list, axis=0)\n",
    "else:\n",
    "    print(\"Feature list is empty.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aef88e7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 10065728 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Process each participant's data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m participant_file \u001b[38;5;129;01min\u001b[39;00m participant_files:\n\u001b[1;32m---> 12\u001b[0m     participant_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(participant_file)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Assuming you have a function to extract features from participant data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     participant_features \u001b[38;5;241m=\u001b[39m extract_features(participant_data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    510\u001b[0m     path,\n\u001b[0;32m    511\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    512\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    513\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    514\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    516\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    221\u001b[0m     path,\n\u001b[0;32m    222\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    223\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    224\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    228\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2973\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2962\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2963\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   2964\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2965\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2970\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   2971\u001b[0m         )\n\u001b[1;32m-> 2973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mread(columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39muse_threads,\n\u001b[0;32m   2974\u001b[0m                         use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[0;32m   2976\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2977\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2979\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   2982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2601\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2593\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2594\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2595\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2596\u001b[0m         ]\n\u001b[0;32m   2597\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2598\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   2599\u001b[0m         )\n\u001b[1;32m-> 2601\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   2602\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_expression,\n\u001b[0;32m   2603\u001b[0m     use_threads\u001b[38;5;241m=\u001b[39muse_threads\n\u001b[0;32m   2604\u001b[0m )\n\u001b[0;32m   2606\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:369\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:2818\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 10065728 failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Collected participant file paths (assuming they are parquet files in the directory)\n",
    "directory = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/\"\n",
    "participant_files = glob.glob(f\"{directory}*.parquet\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "# Processed each participant's data\n",
    "for participant_file in participant_files:\n",
    "    participant_data = pd.read_parquet(participant_file)\n",
    "    \n",
    "    # Assuming you have a function to extract features from participant data\n",
    "    participant_features = extract_features(participant_data)\n",
    "    \n",
    "    if participant_features is not None and not participant_features.empty:\n",
    "        features_list.append(participant_features)\n",
    "    else:\n",
    "        print(f\"Skipping {participant_file} due to empty or invalid features\")\n",
    "\n",
    "# Checked if features were extracted\n",
    "if features_list:\n",
    "    # Concatenate all the individual features\n",
    "    final_features_df = pd.concat(features_list, axis=0)\n",
    "    # Save the concatenated features to a new parquet file\n",
    "    final_features_df.to_parquet(\"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/extracted_features.parquet\")\n",
    "else:\n",
    "    print(\"No features to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a5e19d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample file path for a single participant's data (modify with actual file path)\n",
    "participant_file = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/cleaned_series_train.parquet/id=00f332d1.parquet\"\n",
    "\n",
    "# Function to extract features from the participant data\n",
    "def extract_features(data):\n",
    "    # Feature extraction logic (example)\n",
    "    # Here we calculate a few sample features. You can modify this based on your actual logic.\n",
    "    \n",
    "    # Example: Calculate the mean and standard deviation of selected columns\n",
    "    mean_x = data['X'].mean()\n",
    "    mean_y = data['Y'].mean()\n",
    "    mean_z = data['Z'].mean()\n",
    "    mean_enmo = data['enmo'].mean()\n",
    "    \n",
    "    std_x = data['X'].std()\n",
    "    std_y = data['Y'].std()\n",
    "    std_z = data['Z'].std()\n",
    "    \n",
    "    # Example: Creating a new feature based on activity level (you could customize this logic)\n",
    "    magnitude = np.sqrt(data['X']**2 + data['Y']**2 + data['Z']**2)  # Magnitude of acceleration\n",
    "    activity_level = pd.cut(magnitude, bins=[0, 0.5, 1.0, 1.5, 2.0], labels=['Very Low', 'Low', 'Medium', 'High']).mode()[0]\n",
    "    \n",
    "    # Example: Calculate mean  light and battery voltage\n",
    "    mean_light = data['light'].mean()\n",
    "    mean_battery_voltage = data['battery_voltage'].mean()\n",
    "    \n",
    "    # Created a dictionary with the extracted features\n",
    "    features = {\n",
    "        'mean_x': mean_x,\n",
    "        'mean_y': mean_y,\n",
    "        'mean_z': mean_z,\n",
    "        'mean_enmo': mean_enmo,\n",
    "        'std_x': std_x,\n",
    "        'std_y': std_y,\n",
    "        'std_z': std_z,\n",
    "        'activity_level': activity_level,\n",
    "        'mean_light': mean_light,\n",
    "        'mean_battery_voltage': mean_battery_voltage\n",
    "    }\n",
    "    \n",
    "    # Returned as DataFrame (one row per participant)\n",
    "    return pd.DataFrame([features])\n",
    "\n",
    "# Loaded the participant data (parquet file)\n",
    "participant_data = pd.read_parquet(participant_file)\n",
    "\n",
    "# Extracted features for the participant\n",
    "participant_features = extract_features(participant_data)\n",
    "\n",
    "# Checked if the features are valid\n",
    "if participant_features is not None and not participant_features.empty:\n",
    "    # Save the extracted features to a parquet file (final output for one participant)\n",
    "    participant_features.to_parquet(\"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/extracted_features_id=00f332d1.parquet\")\n",
    "    print(\"Features extracted and saved successfully.\")\n",
    "else:\n",
    "    print(\"No valid features extracted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fba22403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file has been converted to CSV and saved as C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/extracted_features_id=00f332d1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the input Parquet file path and the output CSV file path\n",
    "parquet_file = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/extracted_features_id=00f332d1.parquet\"\n",
    "csv_file = \"C:/Users/Lenovo/OneDrive/Desktop/SEM 2/ml/project/extracted_features_id=00f332d1.csv\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Saved the DataFrame as a CSV file\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Parquet file has been converted to CSV and saved as {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fff89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
